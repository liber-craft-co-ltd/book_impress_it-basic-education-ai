{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5WaFnfv+kLYhWKtwAdlb1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coraldx5/generativeai_intro_book/blob/master/chap05_LLM_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第5章：言語モデルを動かしてみよう\n",
        "- ① 穴埋め問題を解くMLM\n",
        "- ② 次のトークンを予測するCLM"
      ],
      "metadata": {
        "id": "DC7p2jSHrU62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLM（Masked Language Model）を動かてみる\n",
        "- 穴埋め問題を解いてみましょう"
      ],
      "metadata": {
        "id": "q4l2Vi7cDL9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 穴埋め問題を解く準備"
      ],
      "metadata": {
        "id": "Gh3gwy9or2o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi==1.3.2 ipadic==1.0.0 transformers==4.42.4  torch==2.3.1+cu121\n",
        "import transformers, torch\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM, pipeline\n",
        "\n",
        "# トークナイザと訓練済みモデルの読み込み\n",
        "# 'cl-tohoku/bert-base-japanese-whole-word-masking' という事前学習済みの日本語BERTモデルを使用します。\n",
        "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# 事前学習済みモデルに対応するトークナイザーをロードします。\n",
        "# BertJapaneseTokenizerは文章をトークン（モデルが理解できる単位）に変換し、逆にトークンから文章に変換する役割を持ちます。\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "\n",
        "# パイプラインの定義\n",
        "# 'fill-mask' タスクのパイプラインを作成します。これは文章の中の [MASK] トークンを予測するためのものです。\n",
        "fill_mask = pipeline('fill-mask',\n",
        "                     model=model,        # 言語モデルの指定\n",
        "                     tokenizer=tokenizer, # トークナイザの指定\n",
        "                     top_k=6              # 表示する候補数の指定\n",
        "                    )\n",
        "\n",
        "# 結果を表示する関数の定義\n",
        "# 文章を入力として、[MASK] トークンの候補とその確率を表示する関数を定義します。\n",
        "def predictmask(text):\n",
        "    print('---' * 10)\n",
        "    print(f'元の文章：「{text}」')\n",
        "    print(f'[MASK]部の候補：')\n",
        "    for res in fill_mask(text):\n",
        "        print(f\"{res['score']:.4f}: {res['token_str']}\")"
      ],
      "metadata": {
        "id": "mMlwu6MTDg4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 穴埋め問題を解く"
      ],
      "metadata": {
        "id": "fthKi_I8r7fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 例１\n",
        "predictmask('サングラスをかけた[MASK]が公園を駆け回る。')\n",
        "# 例２\n",
        "predictmask('サングラスをかけた[MASK]を食べるのが楽しみだ')\n",
        "# 例３\n",
        "predictmask('生卵をかけた[MASK]を食べるのが楽しみだ')"
      ],
      "metadata": {
        "id": "6sswxk076gC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 参考UI：穴埋め問題を解くサンプルコード\n",
        "# @markdown 入力欄に、[MASK]を含む文章を入力してください。\n",
        "check_sentence = \"サングラスをかけた[MASK]が公園を駆け回る。\" # @param {type:\"string\"}\n",
        "predictmask(check_sentence)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B-7KuJ3RGmkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLM（Causal Language Model）を動かしてみよう\n",
        "- 次のトークンを予測してみよう"
      ],
      "metadata": {
        "id": "aL_wYL7vEABr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文章生成の準備"
      ],
      "metadata": {
        "id": "VloGdE8xupoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, T5Tokenizer\n",
        "\n",
        "# モデルとトークナイザーのロード\n",
        "# 'rinna/japanese-gpt2-medium' という事前学習済みの日本語GPT-2モデルを使用します。モデルのサイズは約1.37GBです。\n",
        "model_name = 'rinna/japanese-gpt2-medium'  # モデルの名前を指定\n",
        "\n",
        "# 事前学習済みのGPT-2モデルをロードします。GPT2LMHeadModelは文章生成タスクに使用されるモデルです。\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# 事前学習済みモデルに対応するトークナイザーをロードします。\n",
        "# T5Tokenizerは文章をトークン（モデルが理解できる単位）に変換し、逆にトークンから文章に変換する役割を持ちます。\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 結果を表示する関数の定義\n",
        "def generate_text(input_text, max_length):\n",
        "    # 入力文章をトークン化\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Attention maskの設定\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "    # 文章生成\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,  # Attention maskを指定\n",
        "        max_length=max_length,  # 生成する最大トークン数\n",
        "        # no_repeat_ngram_size=2,  # 繰り返しを防ぐn-gramのサイズ\n",
        "        pad_token_id=tokenizer.pad_token_id,  # パディングのトークンID\n",
        "        bos_token_id=tokenizer.bos_token_id,  # 文章先頭のトークンID\n",
        "        eos_token_id=tokenizer.eos_token_id,  # 文章終端のトークンID\n",
        "    )\n",
        "\n",
        "    # 生成された文章のデコード\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "VJUG0kK-cMsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 関数の使用例\n",
        "input_text = \"これから雨が降りそうなので、\"  # 入力文章\n",
        "max_length = 50  # 生成する最大トークン数\n",
        "\n",
        "# 生成された文章を表示\n",
        "generated_text = generate_text(input_text, max_length)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "TOJTPWBxEHpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 参考UI：後続文章を生成するサンプルコード\n",
        "# @markdown 文章を入力して下さい\n",
        "input_text = \"これから雨が降りそうなので、\" # @param {type:\"string\"}\n",
        "# @markdown 生成する最大トークン数\n",
        "max_length = 50 # @param {type:\"integer\"}\n",
        "generate_text(input_text, max_length)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NRHw-qPkG4UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 各トークンの予測確率を可視化してみよう"
      ],
      "metadata": {
        "id": "t3Zo7YfeXMZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 貪欲探索の挙動を確認する"
      ],
      "metadata": {
        "id": "IAwX-SgfuzJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを評価モードに設定\n",
        "model.eval()\n",
        "\n",
        "# 入力文章をトークン化\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Attention maskの設定\n",
        "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "# 生成されたトークンとその確率を順次表示\n",
        "max_length = 3  # 生成する最大トークン数\n",
        "for _ in range(max_length):\n",
        "    # トークンの予測確率を取得\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = outputs.logits\n",
        "\n",
        "    # 次のトークンの予測確率を計算\n",
        "    next_token_probs = torch.softmax(predictions[:, -1, :], dim=-1)\n",
        "\n",
        "    # 上位3つのトークンを取得\n",
        "    top_k = 3\n",
        "    top_k_probs, top_k_indices = torch.topk(next_token_probs, top_k)\n",
        "\n",
        "    # 上位3つのトークンとその確率を表示\n",
        "    print(f\"\\n({_+1}番目) 上位3つのトークンと、確率：\")\n",
        "    for i in range(top_k):\n",
        "        predicted_token_id = top_k_indices[0, i].item()\n",
        "        predicted_token = tokenizer.decode([predicted_token_id])\n",
        "        predicted_prob = top_k_probs[0, i].item()\n",
        "        print(f\"Token: {predicted_token}({predicted_token_id}), Probability: {predicted_prob:.4f}\")\n",
        "\n",
        "    # 最も確率の高いトークンを入力トークンに追加\n",
        "    input_ids = torch.cat((input_ids, top_k_indices[:, 0].unsqueeze(-1)), dim=1)\n",
        "\n",
        "    # 予測が終了トークンに到達した場合は終了\n",
        "    if top_k_indices[0, 0].item() == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "# 生成された全文章を表示\n",
        "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "print(\"---\"*10)\n",
        "print(\"生成された文章:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "LCrv7XGhguHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### さまざまな探索手法を体験する"
      ],
      "metadata": {
        "id": "uNNAEwV97iLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルとトークナイザーのロード\n",
        "model_name = 'rinna/japanese-gpt2-medium'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 文章生成の設定\n",
        "input_text = \"これから雨が降りそうなので、\"  # 入力文章\n",
        "max_length = 40  # 生成する最大トークン数\n",
        "\n",
        "# トークン化\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# 文章生成のパラメータを辞書で設定\n",
        "# max_length: 生成する最大トークン数\n",
        "# pad_token_id: パディングのトークンID\n",
        "# bos_token_id: 文章先頭のトークンID\n",
        "# eos_token_id: 文章終端のトークンID\n",
        "prm = {\n",
        "    \"max_length\": max_length,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "    \"bos_token_id\": tokenizer.bos_token_id,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "# Greedy探索\n",
        "# Greedy探索は、各ステップで最も確率の高いトークンを選びます\n",
        "greedy_output = model.generate(input_ids, **prm)\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Beam探索\n",
        "# Beam探索は、複数の候補（ビーム）を同時に探索し、最も良い結果を選びます\n",
        "# num_beams: ビームの数\n",
        "# early_stopping: 生成の早期終了を行うかどうか\n",
        "beam_output = model.generate(input_ids, num_beams=3, early_stopping=True, **prm)\n",
        "print(\"Beam:\", tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-kサンプリング\n",
        "# Top-kサンプリングは、上位k個のトークンからランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_k: 選択する上位トークンの数\n",
        "top_k_output = model.generate(input_ids, do_sample=True, top_k=50, **prm)\n",
        "print(\"Top-k Sampling:\", tokenizer.decode(top_k_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-pサンプリング\n",
        "# Top-pサンプリングは、確率の高いトークンの集合からランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_p: 累積確率がpを超えるまでのトークンを選択する閾値\n",
        "top_p_output = model.generate(input_ids, do_sample=True, top_p=0.95, **prm)\n",
        "print(\"Top-p Sampling:\", tokenizer.decode(top_p_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "avMUcVhTa-kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 参考UI：様々な探索法で文章を生成するサンプルコード\n",
        "# @markdown 文章を入力して下さい\n",
        "# 文章生成の設定\n",
        "input_text = \"これから雨が降りそうなので、\" # @param {type:\"string\"}\n",
        "# @markdown 生成する最大トークン数\n",
        "max_length = 50 # @param {type:\"integer\"}\n",
        "\n",
        "# トークン化\n",
        "# 入力文章をモデルが理解できるトークンに変換します\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# 文章生成のパラメータを辞書で設定\n",
        "# max_length: 生成する最大トークン数\n",
        "# pad_token_id: パディングのトークンID\n",
        "# bos_token_id: 文章先頭のトークンID\n",
        "# eos_token_id: 文章終端のトークンID\n",
        "prm = {\n",
        "    \"max_length\": max_length,\n",
        "    \"pad_token_id\": tokenizer.pad_token_id,\n",
        "    \"bos_token_id\": tokenizer.bos_token_id,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "# Greedy探索\n",
        "# Greedy探索は、各ステップで最も確率の高いトークンを選びます\n",
        "greedy_output = model.generate(input_ids, **prm)\n",
        "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Beam探索\n",
        "# Beam探索は、複数の候補（ビーム）を同時に探索し、最も良い結果を選びます\n",
        "# num_beams: ビームの数\n",
        "# early_stopping: 生成の早期終了を行うかどうか\n",
        "beam_output = model.generate(input_ids, num_beams=3, early_stopping=True, **prm)\n",
        "print(\"Beam:\", tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-kサンプリング\n",
        "# Top-kサンプリングは、上位k個のトークンからランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_k: 選択する上位トークンの数\n",
        "top_k_output = model.generate(input_ids, do_sample=True, top_k=50, **prm)\n",
        "print(\"Top-k Sampling:\", tokenizer.decode(top_k_output[0], skip_special_tokens=True))\n",
        "\n",
        "# Top-pサンプリング\n",
        "# Top-pサンプリングは、確率の高いトークンの集合からランダムに選択します\n",
        "# do_sample: サンプリングを行うかどうか\n",
        "# top_p: 累積確率がpを超えるまでのトークンを選択する閾値\n",
        "top_p_output = model.generate(input_ids, do_sample=True, top_p=0.95, **prm)\n",
        "print(\"Top-p Sampling:\", tokenizer.decode(top_p_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jJuq9RYdksyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 参考：言語モデルをファインチューニングして分類問題を解く"
      ],
      "metadata": {
        "id": "FX8lDcZqiHMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### チューニングに必要な学習データを用意します\n",
        "# 必要なライブラリをインポート\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# データセットの作成\n",
        "# Datasetクラスを継承したクラスを作成します。\n",
        "# ここでは、トークナイズされたデータとそのラベルを受け取って、\n",
        "# データセットを管理する役割を持つクラスです。\n",
        "class CommentDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        # トークナイズされた文章データ（エンコーディング）とラベルを保存します\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        # データセットのサイズ（サンプルの数）を返します\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 指定されたインデックスのデータを取得します\n",
        "        # エンコーディングとラベルをまとめて返します\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "\n",
        "# GitHubからCSVファイルをダウンロードします\n",
        "# wgetはコマンドラインツールで、指定されたURLからファイルをダウンロードします\n",
        "!wget https://raw.githubusercontent.com/coraldx5/generativeai_intro_book/master/movie_review_jpn.csv\n",
        "\n",
        "# データの読み込み\n",
        "# pandasを使ってCSVファイルをデータフレームとして読み込みます\n",
        "df = pd.read_csv(\"movie_review_jpn.csv\")\n",
        "\n",
        "# データの確認\n",
        "# 最初の5行を表示して、データの内容を確認します\n",
        "print(df.head())\n",
        "\n",
        "# ラベルのエンコーディング\n",
        "# 'result'列にはレビューの評価 ('good' または 'bad') が入っているので、\n",
        "# それを数値に変換します。'good'を1、'bad'を0にマップします。\n",
        "df['label'] = df['result'].map({'good': 1, 'bad': 0})\n",
        "\n",
        "# 訓練データとテストデータに分割\n",
        "# データを訓練用とテスト用に分けます。訓練用80%、テスト用20%です。\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# トークナイザーのロード\n",
        "# BERTモデル用の日本語トークナイザーをロードします。これにより、文章がBERTモデルで\n",
        "# 扱える形式にトークナイズ（単語やサブワードに分割）されます。\n",
        "tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "\n",
        "# 文章のトークナイズとエンコード\n",
        "# トークナイザーを使って文章データをBERTモデルに入力できる形式に変換します。\n",
        "def encode_data(data):\n",
        "    return tokenizer.batch_encode_plus(\n",
        "        data['comment'].tolist(),  # コメントをリストとして取得\n",
        "        add_special_tokens=True,    # 特殊トークン（[CLS], [SEP]など）を追加\n",
        "        max_length=128,             # 最大長を128に設定（これ以上は切り捨て）\n",
        "        padding='max_length',       # 最大長に満たない部分をパディング\n",
        "        truncation=True,            # 長すぎるコメントは切り捨て\n",
        "        return_attention_mask=True, # アテンションマスクを返す（BERTの入力に必要）\n",
        "        return_tensors='pt'         # 結果をPyTorchのテンソルとして返す\n",
        "    )\n",
        "\n",
        "# 訓練データとテストデータをエンコードします\n",
        "train_encodings = encode_data(train_df)\n",
        "test_encodings = encode_data(test_df)\n",
        "\n",
        "# ラベルをPyTorchのテンソルに変換します\n",
        "train_labels = torch.tensor(train_df['label'].values)\n",
        "test_labels = torch.tensor(test_df['label'].values)\n",
        "\n",
        "# データセットを作成します\n",
        "train_dataset = CommentDataset(train_encodings, train_labels)\n",
        "test_dataset = CommentDataset(test_encodings, test_labels)\n",
        "\n",
        "# データローダーを作成します\n",
        "# データローダーは、モデルにデータをバッチごとに供給するためのものです。\n",
        "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n"
      ],
      "metadata": {
        "id": "92h1WdJaUqDu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### ファインチューニングを実行します\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# モデルのロード\n",
        "# 事前学習されたBERTモデルをロードし、分類タスク用に設定します\n",
        "# num_labels=2は、ポジティブ/ネガティブの2クラス分類であることを示しています\n",
        "model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese', num_labels=2)\n",
        "\n",
        "# オプティマイザーの設定\n",
        "# AdamWは、重み減衰（Weight Decay）を考慮したAdamオプティマイザーです\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# GPU（CUDA）またはCPUのいずれかを使用するデバイスを設定します\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# モデルを指定したデバイスに転送します\n",
        "model.to(device)\n",
        "\n",
        "# 学習ループを開始します\n",
        "for epoch in range(1):  # ← エポック数（学習量に相当します）\n",
        "    model.train()  # モデルを訓練モードに設定します\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()  # 勾配をゼロにリセットします\n",
        "\n",
        "        # バッチから入力データとラベルを取り出し、デバイスに転送します\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # モデルにデータを入力し、出力と損失を計算します\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # 損失に基づいて勾配を計算し、モデルのパラメータを更新します\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # エポック終了後に損失を表示します\n",
        "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n",
        "\n",
        "    # モデルの評価モードを設定します\n",
        "    model.eval()\n",
        "\n",
        "    # 評価用の変数を初期化します\n",
        "    eval_loss = 0\n",
        "    eval_steps = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # 評価フェーズのループを開始します\n",
        "    with torch.no_grad():  # 評価時には勾配を計算しないので、no_grad()を使用します\n",
        "        for batch in test_loader:\n",
        "            # バッチから入力データとラベルを取り出し、デバイスに転送します\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # モデルにデータを入力し、出力と損失を計算します\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            eval_loss += loss.item()\n",
        "            eval_steps += 1\n",
        "\n",
        "            # 出力のロジットから予測を計算します\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)  # 最も高いスコアを持つラベルを選びます\n",
        "            correct_predictions += (predictions == labels).sum().item()  # 正解した予測の数をカウント\n",
        "            total_predictions += labels.size(0)  # 総ラベル数をカウント\n",
        "\n",
        "    # 評価用データにおける損失と精度を計算して表示します\n",
        "    avg_eval_loss = eval_loss / eval_steps\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Validation Loss: {avg_eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "rRuOJVK0itv9",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## 参考UI：様々な探索法で文章を生成するサンプルコード\n",
        "# @markdown 文章を入力して下さい\n",
        "# 文章生成の設定\n",
        "input_text = \"場面転換が唐突で、ストーリーの流れが断片的に感じられました。\" # @param {type:\"string\"}\n",
        "\n",
        "def predict(text):\n",
        "    # 文章をモデルが理解できる形式にエンコードします\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,                        # 入力文章\n",
        "        add_special_tokens=True,     # 特殊トークン（[CLS], [SEP]など）を追加\n",
        "        max_length=128,              # 最大長を128に設定（これ以上は切り捨て）\n",
        "        padding='max_length',        # 最大長に満たない部分をパディング\n",
        "        truncation=True,             # 長すぎる文章は切り捨て\n",
        "        return_attention_mask=True,  # アテンションマスクを返す（BERTの入力に必要）\n",
        "        return_tensors='pt'          # 結果をPyTorchのテンソルとして返す\n",
        "    )\n",
        "\n",
        "    # エンコードされた入力データをデバイス（GPUまたはCPU）に転送します\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # モデルを評価モードにし、勾配計算を無効にして予測を行います\n",
        "    with torch.no_grad():  # 予測時には勾配を計算しないので、no_grad()を使用します\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # 出力ロジット（モデルの予測結果）から、最も高いスコアを持つクラスを取得します\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()  # 最も高いスコアのインデックスを取得\n",
        "\n",
        "    # クラスに応じて結果を返します（1なら 'good'、それ以外なら 'bad'）\n",
        "    return 'good' if predicted_class == 1 else 'bad'\n",
        "\n",
        "# 予測の例\n",
        "prediction = predict(input_text)\n",
        "print(f\"予測結果: {prediction}\")\n"
      ],
      "metadata": {
        "id": "PI8T1f1QknSC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}